
# BCC Voice & Chat Assistant

AI assistant for the BCC hackathon that:

* **Analyzes client spend & transfers** ‚Üí recommends the best **bank products** (cards, FX, deposit) and **push-messages**.
* **Chats casually** (RU/EN) to guide customers.
* **Fills an FX transfer application** (‚Äú–∑–∞—è–≤–ª–µ–Ω–∏–µ –Ω–∞ –ø–µ—Ä–µ–≤–æ–¥ –≤ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω–æ–π –≤–∞–ª—é—Ç–µ‚Äù) from natural language.
* **Optional voice**: offline **speech-to-text** (Vosk) and **text-to-speech** (pyttsx3).

The recommender reuses the project‚Äôs existing `src/` pipeline (features ‚Üí benefit ‚Üí rank) and supports **time-decayed/recency-aware** signals so offers are relevant **now**.

---

## üóÇ Project layout (important bits)

```
.
‚îú‚îÄ data/                      # (optional) local test files
‚îú‚îÄ results/                   # pipeline outputs (features.csv, recommendations.csv)
‚îú‚îÄ notebooks/                 # scratch / analysis (optional)
‚îú‚îÄ src/
‚îÇ  ‚îú‚îÄ __init__.py
‚îÇ  ‚îú‚îÄ config.py               # categories, product params, defaults
‚îÇ  ‚îú‚îÄ data_loader.py          # ZIP/CSV ‚Üí per-client tables
‚îÇ  ‚îú‚îÄ feature_engineering.py  # build features (totals, % by category, recency, etc.)
‚îÇ  ‚îú‚îÄ benefit_calculator.py   # estimate benefit per product
‚îÇ  ‚îú‚îÄ ranking.py              # pick top-N products
‚îÇ  ‚îú‚îÄ push_generator.py       # rubric-aligned push texts (TOV)
‚îÇ  ‚îî‚îÄ bcc_voice_assistant/
‚îÇ     ‚îú‚îÄ app.py               # ‚≠ê Streamlit app (UI)
‚îÇ     ‚îú‚îÄ assistant/
‚îÇ     ‚îÇ  ‚îú‚îÄ llm.py            # chat via Ollama (fallback rules if not running)
‚îÇ     ‚îÇ  ‚îú‚îÄ slot_filling.py   # extract FX transfer fields from free text
‚îÇ     ‚îÇ  ‚îú‚îÄ forms.py          # Streamlit form + export buttons
‚îÇ     ‚îÇ  ‚îú‚îÄ voice.py          # Vosk STT + pyttsx3 TTS
‚îÇ     ‚îÇ  ‚îî‚îÄ knowledge.py      # short product blurbs & TOV hints
‚îÇ     ‚îî‚îÄ models/              # (optional) put Vosk model here as: models/vosk-model/...
‚îú‚îÄ requirements.txt
‚îî‚îÄ README.md
```

---

## üß∞ Requirements

* **Python** 3.10+
* **pip** (latest)
* (Optional) **Ollama** for local LLM chat: [https://ollama.com](https://ollama.com)
* (Optional) **Vosk** offline STT model (RU or EN), unzipped

### Python deps

```bash
# minimal app
streamlit>=1.37
pandas>=2.0
numpy>=1.24
python-dateutil
pydantic>=2.7
requests

# optional voice/chat extras
pyttsx3          # TTS
vosk             # STT (needs a local model folder)
soundfile        # WAV I/O for Vosk
pywin32          # Windows TTS backend (on Windows)
audio-recorder-streamlit  # mic in Streamlit
```

Install all via the provided `requirements.txt`.

---

## üöÄ Quick start

### 1) Clone & set up a venv

**Windows (PowerShell)**

```powershell
git clone <YOUR_REPO_URL>.git
cd <repo>

py -m venv .venv
& .\.venv\Scripts\Activate.ps1
pip install --upgrade pip
pip install -r requirements.txt
```

**macOS / Linux**

```bash
git clone <YOUR_REPO_URL>.git
cd <repo>

python3 -m venv .venv
source .venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

### 2) (Optional but recommended) Enable real chat with **Ollama**

1. Install Ollama, then pull a model (small & fast):

   ```bash
   ollama pull llama3.1
   ```
2. Run the app after the pull completes. The sidebar shows:
   `LLM: ‚úÖ Ollama ‚Äì llama3.1 @ http://localhost:11434`
   If needed, set:

   ```bash
   # Windows PowerShell
   $env:OLLAMA_URL   = "http://localhost:11434"
   $env:OLLAMA_MODEL = "llama3.1"
   ```

> No Ollama? The assistant **still works** using a rule-based fallback for answers.

### 3) (Optional) Voice mode (offline)

* Download and **unzip** a Vosk model:

  * Russian: `vosk-model-small-ru-0.22`
  * English: `vosk-model-small-en-us-0.15`
* Either:

  1. **Place it inside the repo** as:

     ```
     src/bcc_voice_assistant/models/vosk-model/
       am/
       conf/
       ...
     ```

     (Note: `conf/` and `am/` must be **directly** under `vosk-model/`, not nested deeper.)
  2. **Or** set an env var to wherever you unzipped it:

     ```bash
     # Windows PowerShell
     $env:VOSK_MODEL_PATH = "C:\full\path\to\vosk-model"
     # macOS/Linux
     export VOSK_MODEL_PATH="/full/path/to/vosk-model"
     ```

The app prints **‚ÄúVosk model path resolved to: ‚Ä¶‚Äù** on the Voice tab so you can verify it.

> We **do not** hardcode any machine-specific paths. `voice.py` looks for `VOSK_MODEL_PATH` first and then tries `src/bcc_voice_assistant/models/vosk-model/`.

### 4) Run Streamlit

```bash
# Windows
streamlit run .\src\bcc_voice_assistant\app.py

# macOS/Linux
streamlit run ./src/bcc_voice_assistant/app.py
```

Open the printed URL (usually [http://localhost:8501](http://localhost:8501)).

---

## üí° How to use the app

### Recommender

* **Sidebar ‚Üí Input type**: upload one **ZIP** with all CSVs **or** multiple CSVs.
* Press **Run recommender**.
  You‚Äôll see:

  * `features_streamlit.csv`
  * `recommendations_streamlit.csv`
  * a **push** text for each client‚Äôs top product (TOV: personal ‚Üí benefit ‚Üí single CTA).

### Chat

Ask in RU/EN; examples:

* ‚Äú–ß–∞—Å—Ç–æ —Ç—Ä–∞—á—É –Ω–∞ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã –∏ —Ç–∞–∫—Å–∏ ‚Äî –∫–∞–∫—É—é –∫–∞—Ä—Ç—É –ø–æ—Å–æ–≤–µ—Ç—É–µ—à—å?‚Äù
* ‚Äú–ü–µ—Ä–µ–≤–µ—Å—Ç–∏ 2 400 USD –∑–∞–≤—Ç—Ä–∞. –ü–æ–ª—É—á–∞—Ç–µ–ª—å University of Toronto‚Ä¶ SWIFT ROYCCAT2‚Ä¶‚Äù
* ‚ÄúI usually spend online and on groceries. Compare Shopper vs Everyday.‚Äù

‚öôÔ∏è The assistant:

* auto-fills the **FX transfer application** from what you type/say,
* then you can tweak fields and **download JSON/TXT** of the filled draft.

### Voice (optional)

* Press/hold the mic button, speak, release.
* You‚Äôll see **‚Äú–†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ: ‚Ä¶‚Äù** and hear the answer.
* The same slot-filling logic updates the FX form.

---

## üß† What‚Äôs inside (tech)

* **Feature engineering** (`feature_engineering.py`): totals, **category shares**, inflow/outflow, **recency signals**.
* **Benefit model** (`benefit_calculator.py`): estimates per product (cashback categories, deposit interest, etc.).
* **Ranking** (`ranking.py`): top-N by estimated monthly benefit.
* **Push generation** (`push_generator.py`): follows rubric:

  1. personal context ‚Üí 2) benefit ‚Üí 3) **single CTA**, TOV-conformant (180‚Äì220 chars, no caps).
* **Chat/LLM** (`assistant/llm.py`): uses **Ollama** if available; otherwise a lightweight ruleset.
* **Form fill** (`assistant/slot_filling.py`): robust regex/heuristics + optional LLM JSON extraction.

---

## üîß Troubleshooting

**1) ‚ÄúVosk model path resolved to: .‚Äù / ‚ÄúFolder '.' does not contain model files‚Äù**
‚Üí The model path is empty/invalid. Fix by either:

* putting the unzipped model at `src/bcc_voice_assistant/models/vosk-model/` (must contain `conf/` and `am/`), **or**
* setting `VOSK_MODEL_PATH` to the absolute model folder before running Streamlit.

**2) Voice hears nothing / RU speech with EN model**
Use the model that matches your language. For Russian: `vosk-model-small-ru-0.22`.
For English: `vosk-model-small-en-us-0.15`.

**3) TTS sounds weird (reads ‚ÄúUSD/EUR‚Äù literally)**
We normalize text for TTS and try to pick a RU/EN voice.
On Windows, install a **Russian voice** (Settings ‚Üí Language & region ‚Üí Russian ‚Üí Speech).
You can also install **RHVoice** for higher-quality RU.

**4) Chat gives the same canned answer**
Ollama isn‚Äôt reachable. Make sure:

```bash
ollama pull llama3.1
curl http://localhost:11434/api/tags   # should list llama3.1
```

Sidebar should show `LLM: ‚úÖ Ollama ‚Äì llama3.1`.

**5) CSV upload error (secrets)**
We don‚Äôt require `secrets.toml`. Temporary files are written to the OS temp folder.

**6) Dataframe / Arrow error on `top_products`**
Handled by flattening nested lists into scalar columns before display/export.

---

## üß™ Demo lines (handy)

* ‚Äú–•–æ—á—É –≤—ã–≥–æ–¥–Ω—ã–π –æ–±–º–µ–Ω –≤–∞–ª—é—Ç—ã, —á–∞—Å—Ç–æ –ø–ª–∞—á—É –≤ USD –∏ EUR.‚Äù
* ‚Äú–ß–∞—Å—Ç–æ —Ä–µ—Å—Ç–æ—Ä–∞–Ω—ã –∏ —Ç–∞–∫—Å–∏ ‚Äî –∫–∞–∫—É—é –∫–∞—Ä—Ç—É –ª—É—á—à–µ?‚Äù
* ‚Äú–ü–µ—Ä–µ–≤–æ–¥ 2 150 USD 25.09.2025. –ü–æ–ª—É—á–∞—Ç–µ–ª—å John Smith. –ë–∞–Ω–∫: JPMorgan Chase, –°–®–ê. SWIFT CHASUS33. IBAN US12‚Ä¶ –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: Tuition Fall 2025.‚Äù

---

## üîê Privacy

All processing is **local** (your browser ‚Üî local Streamlit; Ollama runs locally).
No data is sent to external services unless you change the LLM endpoint.

---

## üìù License

Hackathon/demo use. Adapt as needed.

---

### Notes for contributors

* Please keep **no hardcoded absolute paths** in the repo.
* For voice, use `VOSK_MODEL_PATH` or drop a model under `src/bcc_voice_assistant/models/vosk-model/`.
* If you add new product rules, reflect them in `config.py` and `benefit_calculator.py`.
* Keep pushes aligned with the rubric (personal context ‚Üí benefit ‚Üí 1 CTA; TOV).

---

That‚Äôs it‚Äîdrop this as `README.md` in the repo root.
